---
title: "Random Forest - Detection Frequency"
author: "Leah Lariscy"
output: html_document
---

# Load Packages

```{r}
knitr::opts_chunk$set(message=F)
```

```{r}
library(tidyverse)
library(here)
library(tidymodels)
library(tsibble)
library(ingredients)
library(workboots)
```

# Load Data

```{r}
# n = 6 (original data)
data_n6 <- readRDS(here("data/processed_data/wbe_covid_n6_week.rds")) %>% drop_na()

# n = 5
data_n5 <- readRDS(here("data/processed_data/wbe_covid_n5_week.rds")) %>% drop_na()

# n = 4
data_n4 <- readRDS(here("data/processed_data/wbe_covid_n4_week.rds")) %>% drop_na()

# n = 3
data_n3 <- readRDS(here("data/processed_data/wbe_covid_n3_week.rds")) %>% drop_na()

# n = 2
data_n2 <- readRDS(here("data/processed_data/wbe_covid_n2_week.rds")) %>% drop_na()

# n = 1
data_n1 <- readRDS(here("data/processed_data/wbe_covid_n1_week.rds")) %>% drop_na()
```

# Split data

```{r}
data_n6_train <- data_n6 %>% head(n = 75) #training data
data_n6_test <- data_n6 %>% tail(n = 50) #non-training data

data_n5_train <- data_n5 %>% head(n = 75)
data_n5_test <- data_n5 %>% tail(n = 50)

data_n4_train <- data_n4 %>% head(n = 75)
data_n4_test <- data_n4 %>% tail(n = 50)

data_n3_train <- data_n3 %>% head(n = 75)
data_n3_test <- data_n3 %>% tail(n = 50)

data_n2_train <- data_n2 %>% head(n = 75)
data_n2_test <- data_n2 %>% tail(n = 50)

data_n1_train <- data_n1 %>% head(n = 75)
data_n1_test <- data_n1 %>% tail(n = 50)
```

# Model prep

## Define model

```{r}
#define linear regression model
lm_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") 

#define null model
null_model <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("regression")
```

## Define recipes

```{r}
df_n6_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n6_train) %>% 
              update_role(week, new_role = "ID") 

df_n5_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n5_train) %>% 
              update_role(week, new_role = "ID") 

df_n4_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n4_train) %>% 
              update_role(week, new_role = "ID")

df_n3_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n3_train) %>% 
              update_role(week, new_role = "ID")

df_n2_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n2_train) %>% 
              update_role(week, new_role = "ID")

df_n1_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+week,
              data = data_n1_train) %>% 
              update_role(week, new_role = "ID")
```

## Create workflows

```{r}
#workflow for all predictors model (to be tuned)
df_workflow_n6 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n6_recipe)

df_workflow_n5 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n5_recipe)

df_workflow_n4 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n4_recipe)

df_workflow_n3 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n3_recipe)

df_workflow_n2 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n2_recipe)

df_workflow_n1 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(df_n1_recipe)

#workflow for null models
df_null_workflow_n6 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(df_n6_recipe)

df_null_workflow_n1 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(df_n1_recipe)
```

# Model fitting & CV

## n = 6

```{r}
#Training data
#fit model
set.seed(13)
fit_df_n6 <- df_workflow_n6 %>% 
  fit(data_n6_train)

#create folds for cross validation 
set.seed(13)
folds_df_n6 <- vfold_cv(data_n6_train, 
                        v = 10,
                        repeats = 10)

#run cross validation 
set.seed(13)
control <- control_resamples(save_pred = T)
cv_df_n6 <- fit_resamples(df_workflow_n6, resamples = folds_df_n6,
                          control = control)

#calculate performance metrics 
cv_df_n6_metrics <- collect_metrics(cv_df_n6,summarize = F)
df_n6_metrics_sum <- collect_metrics(cv_df_n6,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics 
df_n6_metrics_sum 
```

```{r}
#collect predictions and visualize

#predicted vs observed
collect_predictions(cv_df_n6) %>% 
  ggplot(aes(log10_cases, .pred)) +
  geom_point() +
  stat_poly_line() +
  stat_poly_eq(use_label("eq"))

ggsave(here("figures/model_eval/pred_v_obs/LM_DF.png"))

#residuals
collect_predictions(cv_df_n6) %>% 
  mutate(residual = .pred-log10_cases) %>% 
  ggplot(aes(log10_cases, residual)) +
  geom_point() +
  stat_poly_line() +
  stat_poly_eq(use_label("eq"))

ggsave(here("figures/model_eval/residuals/LM_DF.png"))
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_df_n6_null <- vfold_cv(data_n6_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n6_null <- fit_resamples(df_null_workflow_n6, 
                                resamples = folds_df_n6_null)

#calculate performance metrics
cv_df_n6_metrics_null <- collect_metrics(cv_df_n6_null ,summarize = F)
df_n6_metrics_null_sum <- collect_metrics(cv_df_n6_null ,summarize = T)

#check cross validation metrics
df_n6_metrics_null_sum
```

## n = 5

```{r}
#fit model to data
set.seed(13)
fit_df_n5 <- df_workflow_n5 %>% 
  fit(data = data_n5_train)

#create folds for cross validation
set.seed(13)
folds_df_n5 <- vfold_cv(data_n5_train, 
                        v = 10,
                        repeats = 10)


#run cross validation
set.seed(13)
cv_df_n5 <- fit_resamples(df_workflow_n5, resamples = folds_df_n5)

#calculate performance metrics
cv_df_n5_metrics <- collect_metrics(cv_df_n5,summarize = F)
df_n5_metrics_sum <- collect_metrics(cv_df_n5,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n5_metrics_sum 
```

## n = 4

```{r}
#fit model to data
set.seed(13)
fit_df_n4 <- df_workflow_n4 %>% 
  fit(data = data_n4_train)

#create folds for cross validation
set.seed(13)
folds_df_n4 <- vfold_cv(data_n4_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n4 <- fit_resamples(df_workflow_n4, resamples = folds_df_n4)

#calculate performance metrics
cv_df_n4_metrics <- collect_metrics(cv_df_n4,summarize = F)
df_n4_metrics_sum <- collect_metrics(cv_df_n4,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n4_metrics_sum
```

## n = 3

```{r}
#fit model to data
set.seed(13)
fit_df_n3 <- df_workflow_n3 %>% 
  fit(data = data_n3_train)

#create folds for cross validation
set.seed(13)
folds_df_n3 <- vfold_cv(data_n3_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n3 <- fit_resamples(df_workflow_n3, resamples = folds_df_n3)

#calculate performance metrics
cv_df_n3_metrics <- collect_metrics(cv_df_n3,summarize = F)
df_n3_metrics_sum <- collect_metrics(cv_df_n3,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n3_metrics_sum 
```

## n = 2

```{r}
#fit model to data
set.seed(13)
fit_df_n2 <- df_workflow_n2 %>% 
  fit(data = data_n2_train)

#create folds for cross validation
set.seed(13)
folds_df_n2 <- vfold_cv(data_n2_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n2 <- fit_resamples(df_workflow_n2, resamples = folds_df_n2)

#calculate performance metrics
cv_df_n2_metrics <- collect_metrics(cv_df_n2,summarize = F)
df_n2_metrics_sum <- collect_metrics(cv_df_n2,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n2_metrics_sum 
```

## n = 1

```{r}
#fit model to data
set.seed(13)
fit_df_n1 <- df_workflow_n1 %>% 
  fit(data = data_n1_train)

#create folds for cross validation
set.seed(13)
folds_df_n1 <- vfold_cv(data_n1_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n1 <- fit_resamples(df_workflow_n1, resamples = folds_df_n1)

#calculate performance metrics
cv_df_n1_metrics <- collect_metrics(cv_df_n1,summarize = F)
df_n1_metrics_sum <- collect_metrics(cv_df_n1,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n1_metrics_sum 
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_df_n1_null <- vfold_cv(data_n1_train, 
                        v = 10,
                        repeats = 10)

#run cross validation
set.seed(13)
cv_df_n1_null <- fit_resamples(df_null_workflow_n1, 
                                resamples = folds_df_n1_null)

#calculate performance metrics
cv_df_n1_metrics_null <- collect_metrics(cv_df_n1_null,summarize = F)
df_n1_metrics_null_sum <- collect_metrics(cv_df_n1_null,summarize = T) %>% 
  mutate(lower_ci = mean-1.96*std_err,
         upper_ci = mean+1.96*std_err)

#check cross validation metrics
df_n1_metrics_null_sum
```

## Export results

```{r}
#export summarized metrics
df_n6_metrics_sum <- df_n6_metrics_sum %>% mutate(.config="df_n6",
                                                feature="df")
df_n5_metrics_sum <- df_n5_metrics_sum %>% mutate(.config="df_n5",
                                                feature="df")
df_n4_metrics_sum <- df_n4_metrics_sum %>% mutate(.config="df_n4",
                                                feature="df")
df_n3_metrics_sum <- df_n3_metrics_sum %>% mutate(.config="df_n3",
                                                feature="df")
df_n2_metrics_sum <- df_n2_metrics_sum %>% mutate(.config="df_n2",
                                                feature="df")
df_n1_metrics_sum <- df_n1_metrics_sum %>% mutate(.config="df_n1",
                                                feature="df")

cv_metrics_df <- rbind(df_n1_metrics_sum,
                       df_n2_metrics_sum,
                       df_n3_metrics_sum,
                       df_n4_metrics_sum,
                       df_n5_metrics_sum,
                       df_n6_metrics_sum)

saveRDS(cv_metrics_df, 
        here("data/processed_data/linear_reg/cv_metrics_df.rds"))

#export unsummarized metrics
cv_df_n6_metrics <- cv_df_n6_metrics %>% mutate(.config="n6",
                                                feature="df")
cv_df_n5_metrics <- cv_df_n5_metrics %>% mutate(.config="n5",
                                                feature="df")
cv_df_n4_metrics <- cv_df_n4_metrics %>% mutate(.config="n4",
                                                feature="df")
cv_df_n3_metrics <- cv_df_n3_metrics %>% mutate(.config="n3",
                                                feature="df")
cv_df_n2_metrics <- cv_df_n2_metrics %>% mutate(.config="n2",
                                                feature="df")
cv_df_n1_metrics <- cv_df_n1_metrics %>% mutate(.config="n1",
                                                feature="df")

cv_metrics_df_raw <- rbind(cv_df_n6_metrics,
                       cv_df_n5_metrics,
                       cv_df_n4_metrics,
                       cv_df_n3_metrics,
                       cv_df_n2_metrics,
                       cv_df_n1_metrics)

saveRDS(cv_metrics_df_raw, 
        here("data/processed_data/linear_reg/cv_metrics_df_raw.rds"))
```

# Observed vs predicted

## Training data (2020-2021)

```{r}
# generate predictions from 2000 bootstrap models
set.seed(456)
n3_pred_conf_int_train <- 
  df_workflow_n3 %>%
  predict_boots(
    n = 2000,
    training_data = data_n3_train,
    new_data = data_n3_train,
    interval = "confidence"
  )

# summarise with a 95% confidence interval
n3_pred_conf_int_train_sum <- n3_pred_conf_int_train %>%
  summarise_predictions()

#select important variables from original data
n3_actual_train <- data_n3_train %>% select(week,log10_cases)

#select important variables from predicted data
n3_pred_train <- n3_pred_conf_int_train_sum %>% select(.pred,.pred_lower,.pred_upper)

#bind original and predicted data and undo log transformation
n3_train_compare <- bind_cols(n3_actual_train, n3_pred_train) %>% 
  mutate(prediction = 10^.pred,
       actual = 10^log10_cases,
       pred_lower = 10^.pred_lower,
       pred_upper = 10^.pred_upper) %>% 
  select(week,actual,prediction,pred_lower,pred_upper)

#calculate R-squared and RMSE
n3_train_rsq <- n3_train_compare %>% 
  rsq(truth = actual, estimate = prediction)

n3_train_rsq

n3_train_rmse <- n3_train_compare %>% 
  rmse(truth = actual,estimate = prediction)

n3_train_rmse

#visualize timeseries of actual vs predicted data
n3_train_compare %>% 
  ggplot(aes(x=week)) +
  geom_line(aes(y=prediction), color = "blue") +
  geom_point(aes(y=prediction), color = "blue") +
  geom_ribbon(aes(ymin=pred_lower, ymax=pred_upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()

#export files for use in comparison script
saveRDS(n3_train_compare,here(
  "data/processed_data/linear_reg/df_n3_train_predictions.rds"))

saveRDS(n3_train_rsq,here(
  "data/processed_data/linear_reg/df_n3_train_rsq.rds"))

saveRDS(n3_train_rmse,here(
  "data/processed_data/linear_reg/df_n3_train_rmse.rds"))
```

## Testing data (2022)

```{r}
# generate predictions from 2000 bootstrap models
set.seed(456)
n3_pred_conf_int_test <- 
  df_workflow_n3 %>%
  predict_boots(
    n = 2000,
    training_data = data_n3_train,
    new_data = data_n3_test,
    interval = "confidence"
  )

# summarise with a 95% confidence interval
n3_pred_conf_int_test_sum <- n3_pred_conf_int_test %>%
  summarise_predictions()

#select important variables from original data
n3_actual_test <- data_n3_test %>% select(week,log10_cases)

#select important variables from predicted data
n3_pred_test <- n3_pred_conf_int_test_sum %>% select(.pred,.pred_lower,.pred_upper)

#bind original and predicted data and undo log transformation
n3_test_compare <- bind_cols(n3_actual_test, n3_pred_test) %>% 
  mutate(prediction = 10^.pred,
       actual = 10^log10_cases,
       pred_lower = 10^.pred_lower,
       pred_upper = 10^.pred_upper) %>% 
  select(week,actual,prediction,pred_lower,pred_upper)

#calculate R-squared and RMSE
n3_test_rsq <- n3_test_compare %>% 
  rsq(truth = actual, estimate = prediction)

n3_test_rsq

n3_test_rmse <- n3_test_compare %>% 
  rmse(truth = actual,estimate = prediction)

n3_test_rmse

#visualize timeseries of actual vs predicted data
n3_test_compare %>% 
  ggplot(aes(x=week)) +
  geom_line(aes(y=prediction), color = "blue") +
  geom_point(aes(y=prediction), color = "blue") +
  geom_ribbon(aes(ymin=pred_lower, ymax=pred_upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()

#export files for use in comparison script
saveRDS(n3_test_compare,here(
  "data/processed_data/linear_reg/df_n3_test_predictions.rds"))

saveRDS(n3_test_rsq,here(
  "data/processed_data/linear_reg/df_n3_test_rsq.rds"))

saveRDS(n3_test_rmse,here(
  "data/processed_data/linear_reg/df_n3_test_rmse.rds"))
```

## Full timeseries

```{r}
# generate predictions from 2000 bootstrap models
set.seed(456)
n3_pred_conf_int_full <- 
  df_workflow_n3 %>%
  predict_boots(
    n = 2000,
    training_data = data_n3_train,
    new_data = data_n3,
    interval = "confidence"
  )

# summarise with a 95% confidence interval
n3_pred_conf_int_full_sum <- n3_pred_conf_int_full %>%
  summarise_predictions()

#select important variables from original data
n3_actual_full <- data_n3 %>% select(week,log10_cases)

#select important variables from predicted data
n3_pred_full <- n3_pred_conf_int_full_sum %>% select(.pred,.pred_lower,.pred_upper)

#bind original and predicted data and undo log transformation
n3_full_compare <- bind_cols(n3_actual_full, n3_pred_full) %>% 
  mutate(prediction = 10^.pred,
       actual = 10^log10_cases,
       pred_lower = 10^.pred_lower,
       pred_upper = 10^.pred_upper) %>% 
  select(week,actual,prediction,pred_lower,pred_upper)

n3_full_compare %>% 
  ggplot(aes(x=week)) +
  geom_line(aes(y=prediction), color = "blue") +
  geom_point(aes(y=prediction), color = "blue") +
  geom_ribbon(aes(ymin=pred_lower, ymax=pred_upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()

#export files for use in comparison script
saveRDS(n3_full_compare,here(
  "data/processed_data/linear_reg/df_n3_full_predictions.rds"))
```
